{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6cf4f5c",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "\n",
    "BRAUX Owen and CAMBIER Elliot\n",
    "\n",
    "    This notebook combines on-chain transaction data with market price data to create features for a prediction model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd40ecab",
   "metadata": {},
   "source": [
    "## Set up spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93061489",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, from_unixtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928f88c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/11/15 10:16:41 WARN Utils: Your hostname, OBPC, resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)\n",
      "25/11/15 10:16:41 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/11/15 10:16:43 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"BDA - Feature Engineering\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0efc385a",
   "metadata": {},
   "source": [
    "## Load Prepared Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b09851",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Timestamp: double (nullable = true)\n",
      " |-- Open: double (nullable = true)\n",
      " |-- High: double (nullable = true)\n",
      " |-- Low: double (nullable = true)\n",
      " |-- Close: double (nullable = true)\n",
      " |-- Volume: double (nullable = true)\n",
      "\n",
      "\n",
      " Cleaned Price Data Schema and Sample :\n",
      "root\n",
      " |-- unix_timestamp: double (nullable = true)\n",
      " |-- price_open: double (nullable = true)\n",
      " |-- price_high: double (nullable = true)\n",
      " |-- price_low: double (nullable = true)\n",
      " |-- price_close: double (nullable = true)\n",
      " |-- volume_btc: double (nullable = true)\n",
      " |-- timestamp_utc: timestamp (nullable = true)\n",
      "\n",
      "+-------------------+----------+-----------+----------+\n",
      "|      timestamp_utc|price_open|price_close|volume_btc|\n",
      "+-------------------+----------+-----------+----------+\n",
      "|2012-01-01 11:01:00|      4.58|       4.58|       0.0|\n",
      "|2012-01-01 11:02:00|      4.58|       4.58|       0.0|\n",
      "|2012-01-01 11:03:00|      4.58|       4.58|       0.0|\n",
      "|2012-01-01 11:04:00|      4.58|       4.58|       0.0|\n",
      "|2012-01-01 11:05:00|      4.58|       4.58|       0.0|\n",
      "+-------------------+----------+-----------+----------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# Load the market price data of notebook 1\n",
    "price_data_path = \"../data/prices/btcusd_1-min_data.csv\"\n",
    "df_prices_raw = spark.read.csv(price_data_path, header=True, inferSchema=True)\n",
    "df_prices_raw.printSchema()\n",
    "\n",
    "# Clean up the price data columns\n",
    "df_prices = df_prices_raw.withColumnRenamed(\"Timestamp\", \"unix_timestamp\") \\\n",
    "                         .withColumnRenamed(\"Open\", \"price_open\") \\\n",
    "                         .withColumnRenamed(\"High\", \"price_high\") \\\n",
    "                         .withColumnRenamed(\"Low\", \"price_low\") \\\n",
    "                         .withColumnRenamed(\"Close\", \"price_close\") \\\n",
    "                         .withColumnRenamed(\"Volume\", \"volume_btc\")\n",
    "\n",
    "if \"Volume_(Currency)\" in df_prices.columns:\n",
    "    df_prices = df_prices.withColumnRenamed(\"Volume_(Currency)\", \"volume_currency\")\n",
    "\n",
    "if \"Weighted_Price\" in df_prices.columns:\n",
    "    df_prices = df_prices.withColumnRenamed(\"Weighted_Price\", \"weighted_price\")\n",
    "\n",
    "\n",
    "# Convert Unix timestamp to a proper timestamp type\n",
    "df_prices = df_prices.withColumn(\"timestamp_utc\", from_unixtime(col(\"unix_timestamp\")).cast(\"timestamp\"))\n",
    "\n",
    "print(\"\\n Cleaned Price Data Schema and Sample :\")\n",
    "df_prices.printSchema()\n",
    "df_prices.select(\"timestamp_utc\", \"price_open\", \"price_close\", \"volume_btc\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701de7d1",
   "metadata": {},
   "source": [
    "## Aggregate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9d55a14-24fa-4371-9203-3a3190721f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import window, sum, count, avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46907148-a18d-46d5-a97f-0302252bccff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " data schema :\n",
      "root\n",
      " |-- block_timestamp: long (nullable = true)\n",
      " |-- n_inputs: integer (nullable = true)\n",
      " |-- n_outputs: integer (nullable = true)\n",
      " |-- total_amount_satoshi: long (nullable = true)\n",
      " |-- total_amount_btc: double (nullable = true)\n",
      " |-- timestamp_utc: timestamp (nullable = true)\n",
      "\n",
      "+---------------+--------+---------+--------------------+----------------+-------------------+\n",
      "|block_timestamp|n_inputs|n_outputs|total_amount_satoshi|total_amount_btc|      timestamp_utc|\n",
      "+---------------+--------+---------+--------------------+----------------+-------------------+\n",
      "|     1339713349|       1|        1|          5025512500|       50.255125|2012-06-15 00:35:49|\n",
      "|     1339713349|       4|        2|          9779326418|     97.79326418|2012-06-15 00:35:49|\n",
      "|     1339713349|       3|        2|         14502850000|        145.0285|2012-06-15 00:35:49|\n",
      "|     1339713349|       1|        1|           100000000|             1.0|2012-06-15 00:35:49|\n",
      "|     1339713349|       2|        2|          1009429329|     10.09429329|2012-06-15 00:35:49|\n",
      "+---------------+--------+---------+--------------------+----------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "--- On-Chain Features Schema and Sample ---\n",
      "root\n",
      " |-- window: struct (nullable = false)\n",
      " |    |-- start: timestamp (nullable = true)\n",
      " |    |-- end: timestamp (nullable = true)\n",
      " |-- tx_count: long (nullable = false)\n",
      " |-- tx_volume_btc: double (nullable = true)\n",
      " |-- avg_inputs: double (nullable = true)\n",
      " |-- avg_outputs: double (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6:===================================================>       (7 + 1) / 8]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------+--------+------------------+------------------+------------------+\n",
      "|window                                    |tx_count|tx_volume_btc     |avg_inputs        |avg_outputs       |\n",
      "+------------------------------------------+--------+------------------+------------------+------------------+\n",
      "|{2012-06-10 00:54:00, 2012-06-10 00:55:00}|87      |750.3537802699998 |2.4597701149425286|1.9540229885057472|\n",
      "|{2012-06-10 01:54:00, 2012-06-10 01:55:00}|132     |1869.5332449499995|1.7954545454545454|1.946969696969697 |\n",
      "|{2012-06-10 03:25:00, 2012-06-10 03:26:00}|189     |977.1499989299997 |1.8783068783068784|2.5396825396825395|\n",
      "|{2012-06-10 05:14:00, 2012-06-10 05:15:00}|554     |4660.899853159999 |2.108303249097473 |3.267148014440433 |\n",
      "|{2012-06-10 06:46:00, 2012-06-10 06:47:00}|233     |1512.524592979997 |1.793991416309013 |2.3433476394849784|\n",
      "|{2012-06-10 08:18:00, 2012-06-10 08:19:00}|81      |3584.4071749400005|1.5925925925925926|1.9876543209876543|\n",
      "|{2012-06-10 10:17:00, 2012-06-10 10:18:00}|512     |10252.806667159974|1.46875           |2.10546875        |\n",
      "|{2012-06-10 11:18:00, 2012-06-10 11:19:00}|420     |12563.07004679    |3.216666666666667 |2.0761904761904764|\n",
      "|{2012-06-10 12:08:00, 2012-06-10 12:09:00}|503     |2673.2388218700025|2.0596421471172963|2.0258449304174952|\n",
      "|{2012-06-10 14:15:00, 2012-06-10 14:16:00}|6       |3416.3747347799995|3.0               |1.8333333333333333|\n",
      "+------------------------------------------+--------+------------------+------------------+------------------+\n",
      "only showing top 10 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# load the parquet file\n",
    "transactions_path = \"../data/processed/transactions.parquet\"\n",
    "df_transactions = spark.read.parquet(transactions_path)\n",
    "\n",
    "print(\"\\n data schema :\")\n",
    "df_transactions.printSchema()\n",
    "df_transactions.show(5)\n",
    "\n",
    "# Aggregate transaction data into 1-minute windows\n",
    "# (We group transactions by time windows to match the granularity of our price data)\n",
    "onchain_features_df = df_transactions.groupBy(\n",
    "    # 'window' creates tumbling (non-overlapping) windows of a specified duration.\n",
    "    window(col(\"timestamp_utc\"), \"1 minute\")\n",
    ").agg(\n",
    "    count(\"*\").alias(\"tx_count\"),\n",
    "    sum(\"total_amount_btc\").alias(\"tx_volume_btc\"),\n",
    "    avg(\"n_inputs\").alias(\"avg_inputs\"),\n",
    "    avg(\"n_outputs\").alias(\"avg_outputs\")\n",
    ")\n",
    "\n",
    "print(\"\\n--- On-Chain Features Schema and Sample ---\")\n",
    "onchain_features_df.printSchema()\n",
    "\n",
    "# Show the results, sorting by the window time\n",
    "onchain_features_df.sort(\"window\").show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fbf5a18-07bc-4430-9bb2-d3f8fbd70e4d",
   "metadata": {},
   "source": [
    "## Join On-Chain Features with Market Prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c922756-75b1-436c-98d5-4f995fe8885b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a36b433-b018-4370-9fe2-8e7413125297",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joining price data with on-chain features...\n",
      "root\n",
      " |-- timestamp_utc: timestamp (nullable = true)\n",
      " |-- unix_timestamp: double (nullable = true)\n",
      " |-- price_open: double (nullable = true)\n",
      " |-- price_high: double (nullable = true)\n",
      " |-- price_low: double (nullable = true)\n",
      " |-- price_close: double (nullable = true)\n",
      " |-- volume_btc: double (nullable = true)\n",
      " |-- tx_count: long (nullable = true)\n",
      " |-- tx_volume_btc: double (nullable = false)\n",
      " |-- avg_inputs: double (nullable = false)\n",
      " |-- avg_outputs: double (nullable = false)\n",
      "\n",
      "\n",
      " Sample of the Final Combined DataFrame : \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------+--------+-------------+\n",
      "|      timestamp_utc|price_close|tx_count|tx_volume_btc|\n",
      "+-------------------+-----------+--------+-------------+\n",
      "|2025-10-14 01:56:00|   115242.0|       0|          0.0|\n",
      "|2025-10-14 01:55:00|   115217.0|       0|          0.0|\n",
      "|2025-10-14 01:54:00|   115221.0|       0|          0.0|\n",
      "|2025-10-14 01:53:00|   115218.0|       0|          0.0|\n",
      "|2025-10-14 01:52:00|   115232.0|       0|          0.0|\n",
      "|2025-10-14 01:51:00|   115213.0|       0|          0.0|\n",
      "|2025-10-14 01:50:00|   115211.0|       0|          0.0|\n",
      "|2025-10-14 01:49:00|   115174.0|       0|          0.0|\n",
      "|2025-10-14 01:48:00|   115159.0|       0|          0.0|\n",
      "|2025-10-14 01:47:00|   115107.0|       0|          0.0|\n",
      "|2025-10-14 01:46:00|   115109.0|       0|          0.0|\n",
      "|2025-10-14 01:45:00|   115134.0|       0|          0.0|\n",
      "|2025-10-14 01:44:00|   115282.0|       0|          0.0|\n",
      "|2025-10-14 01:43:00|   115293.0|       0|          0.0|\n",
      "|2025-10-14 01:42:00|   115265.0|       0|          0.0|\n",
      "+-------------------+-----------+--------+-------------+\n",
      "only showing top 15 rows\n",
      "Total rows in price data: 7,248,636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 23:>                                                         (0 + 8) / 8]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows in combined data: 7,248,636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Prepare the data for the join.\n",
    "#  renamed to 'timestamp_utc' to match the price df column\n",
    "onchain_features_to_join = onchain_features_df.withColumn(\"timestamp_utc\", col(\"window.start\")) \\\n",
    "                                              .drop(\"window\")\n",
    "\n",
    "# join both df\n",
    "print(\"Joining price data with on-chain features...\")\n",
    "df_combined = df_prices.join(\n",
    "    onchain_features_to_join,\n",
    "    on=\"timestamp_utc\",  # The common column to join on\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# cleaning we replace nulls with 0\n",
    "feature_columns = [\"tx_count\", \"tx_volume_btc\", \"avg_inputs\", \"avg_outputs\"]\n",
    "df_combined = df_combined.fillna(0, subset=feature_columns)\n",
    "\n",
    "df_combined.printSchema() # Quick check\n",
    "\n",
    "print(\"\\n Sample of the Final Combined DataFrame : \")\n",
    "df_combined.select(\n",
    "    \"timestamp_utc\",\n",
    "    \"price_close\",\n",
    "    \"tx_count\",\n",
    "    \"tx_volume_btc\"\n",
    ").sort(\"timestamp_utc\", ascending=False).show(15)\n",
    "\n",
    "# Count the rows to make sure we didn't lose our price data\n",
    "print(f\"Total rows in price data: {df_prices.count():,}\")\n",
    "print(f\"Total rows in combined data: {df_combined.count():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16a2624c-564e-4e21-8cb8-097e3dc83b9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Inspecting the combined data around June 2012 :\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 33:===============>  (7 + 1) / 8][Stage 34:===============>  (7 + 1) / 8]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------+--------+------------------+\n",
      "|      timestamp_utc|price_close|tx_count|     tx_volume_btc|\n",
      "+-------------------+-----------+--------+------------------+\n",
      "|2012-06-15 00:30:00|       5.87|       0|               0.0|\n",
      "|2012-06-15 00:31:00|       5.87|       0|               0.0|\n",
      "|2012-06-15 00:32:00|       5.87|       0|               0.0|\n",
      "|2012-06-15 00:33:00|       5.87|       0|               0.0|\n",
      "|2012-06-15 00:34:00|       5.87|       0|               0.0|\n",
      "|2012-06-15 00:35:00|       5.87|     413|1413.6811811800012|\n",
      "|2012-06-15 00:36:00|       5.87|     263|1029.1968248699995|\n",
      "|2012-06-15 00:37:00|       5.87|       0|               0.0|\n",
      "|2012-06-15 00:38:00|       5.87|       0|               0.0|\n",
      "|2012-06-15 00:39:00|       5.87|       0|               0.0|\n",
      "|2012-06-15 00:40:00|       5.87|       0|               0.0|\n",
      "|2012-06-15 00:41:00|       5.87|       0|               0.0|\n",
      "|2012-06-15 00:42:00|       5.87|       0|               0.0|\n",
      "|2012-06-15 00:43:00|       5.87|       0|               0.0|\n",
      "|2012-06-15 00:44:00|       5.87|       0|               0.0|\n",
      "|2012-06-15 00:45:00|       5.87|       0|               0.0|\n",
      "+-------------------+-----------+--------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "print(\" Inspecting the combined data around June 2012 :\")\n",
    "\n",
    "df_combined.filter(\"timestamp_utc between '2012-06-15 00:30:00' and '2012-06-15 00:45:00'\") \\\n",
    "           .select(\"timestamp_utc\", \"price_close\", \"tx_count\", \"tx_volume_btc\") \\\n",
    "           .sort(\"timestamp_utc\") \\\n",
    "           .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7197561d-c7b5-40df-9bf9-2a02dd1f0e17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 50:==================================================>       (7 + 1) / 8]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving in parquet format Done !\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "df_combined.write.mode(\"overwrite\").parquet(\"../data/processed/features.parquet\")\n",
    "print(\"Saving in parquet format Done !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c84230e-2c16-4a38-aa0e-cd9c68da836e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
